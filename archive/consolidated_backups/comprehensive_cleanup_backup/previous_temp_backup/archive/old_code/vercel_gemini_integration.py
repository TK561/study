#!/usr/bin/env python3
"""
Vercel Ã— Gemini AI çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ 
AIã«ã‚ˆã‚‹è‡ªå‹•æœ€é©åŒ–ã¨äºˆæ¸¬çš„ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆç®¡ç†
"""

import json
import os
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ä¾å­˜é–¢ä¿‚
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("ğŸ“ Note: Gemini AIæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ 'pip install google-generativeai' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("ğŸ“ Note: .envæ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ 'pip install python-dotenv' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from vercel_deployment_manager import VercelDeploymentManager
from vercel_update_tracker import VercelUpdateTracker
from vercel_fix_assistant import VercelFixAssistant

class VercelGeminiIntegration:
    def __init__(self):
        # Gemini APIè¨­å®š
        self.gemini_api_key = os.getenv('GEMINI_API_KEY') if 'os' in globals() else None
        
        if GEMINI_AVAILABLE and self.gemini_api_key:
            genai.configure(api_key=self.gemini_api_key)
            self.model = genai.GenerativeModel('gemini-pro')
        else:
            self.model = None
            if not GEMINI_AVAILABLE:
                print("ğŸ“ Gemini AIæ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
            elif not self.gemini_api_key:
                print("ğŸ“ Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
        
        # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆ
        self.deployment_manager = VercelDeploymentManager()
        self.update_tracker = VercelUpdateTracker()
        self.fix_assistant = VercelFixAssistant()
        
        # AIå­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        self.ai_insights_file = "VERCEL_AI_INSIGHTS.json"
        self.user_satisfaction_file = "VERCEL_USER_SATISFACTION.json"
        self.ai_insights = self._load_ai_insights()
    
    def _load_ai_insights(self) -> Dict:
        """AIæ´å¯Ÿãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        if os.path.exists(self.ai_insights_file):
            with open(self.ai_insights_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "deployment_patterns": [],
            "optimization_history": [],
            "user_preferences": {},
            "success_factors": []
        }
    
    def _save_ai_insights(self):
        """AIæ´å¯Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        with open(self.ai_insights_file, 'w', encoding='utf-8') as f:
            json.dump(self.ai_insights, f, ensure_ascii=False, indent=2)
    
    async def analyze_deployment_with_ai(self, deployment_config: Dict) -> Dict:
        """Gemini AIã§ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ§‹æˆã‚’åˆ†æ"""
        if not self.model:
            return {"ai_available": False, "reason": "Gemini APIæœªè¨­å®š"}
        
        # éå»ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
        success_patterns = self.deployment_manager.success_patterns.get("patterns", [])
        
        # AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt = f"""
        Vercelãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ§‹æˆã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

        ç¾åœ¨ã®æ§‹æˆ:
        {json.dumps(deployment_config, ensure_ascii=False, indent=2)}

        éå»ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³:
        {json.dumps(success_patterns[-3:], ensure_ascii=False, indent=2)}

        ä»¥ä¸‹ã®è¦³ç‚¹ã§åˆ†æã—ã¦ãã ã•ã„ï¼š
        1. æˆåŠŸç¢ºç‡ã®äºˆæ¸¬ï¼ˆ0-100%ï¼‰
        2. æ½œåœ¨çš„ãªå•é¡Œç‚¹
        3. æœ€é©åŒ–ã®ææ¡ˆ
        4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®æ”¹å–„ç‚¹
        5. æ¨å¥¨ã•ã‚Œã‚‹æ§‹æˆå¤‰æ›´

        JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        """
        
        try:
            response = await self.model.generate_content_async(prompt)
            analysis = json.loads(response.text)
            
            # AIæ´å¯Ÿã‚’è¨˜éŒ²
            self.ai_insights["deployment_patterns"].append({
                "timestamp": datetime.now().isoformat(),
                "config": deployment_config,
                "ai_analysis": analysis
            })
            self._save_ai_insights()
            
            return analysis
            
        except Exception as e:
            print(f"âš ï¸ AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"ai_available": False, "error": str(e)}
    
    def predict_deployment_success(self, config: Dict) -> Tuple[float, List[str]]:
        """ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆåŠŸç‡ã‚’äºˆæ¸¬"""
        success_rate = 0.0
        factors = []
        
        # åŸºæœ¬ãƒã‚§ãƒƒã‚¯
        if config.get("type") == "static_html":
            success_rate += 40
            factors.append("é™çš„HTMLã‚µã‚¤ãƒˆï¼ˆ+40%ï¼‰")
        
        if "api/" not in str(config.get("files", [])):
            success_rate += 20
            factors.append("APIä¸ä½¿ç”¨ï¼ˆ+20%ï¼‰")
        
        # éå»ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®é¡ä¼¼æ€§
        similar_pattern = self.deployment_manager.find_similar_success_pattern(config)
        if similar_pattern:
            success_rate += 30
            factors.append(f"é¡ä¼¼æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚ã‚Šï¼ˆ+30%ï¼‰")
        
        # æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼å±¥æ­´ç¢ºèª
        if hasattr(self.fix_assistant, 'error_history'):
            recent_errors = len([e for e in self.fix_assistant.error_history 
                               if e.get("timestamp", "") > 
                               (datetime.now() - timedelta(hours=24)).isoformat()])
            if recent_errors == 0:
                success_rate += 10
                factors.append("24æ™‚é–“ã‚¨ãƒ©ãƒ¼ãªã—ï¼ˆ+10%ï¼‰")
        
        return min(success_rate, 100), factors
    
    def optimize_deployment_config(self, current_config: Dict) -> Dict:
        """ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ§‹æˆã‚’æœ€é©åŒ–"""
        optimized = current_config.copy()
        optimizations = []
        
        # é™çš„ã‚µã‚¤ãƒˆæœ€é©åŒ–
        if current_config.get("type") != "static_html" and "api/" in str(current_config.get("files", [])):
            optimized["type"] = "static_html"
            optimized["recommendation"] = "é™çš„HTMLã‚µã‚¤ãƒˆã¸ã®ç§»è¡Œã‚’æ¨å¥¨"
            optimizations.append("APIã‚’é™çš„HTMLã«å¤‰æ›")
        
        # vercel.jsonæœ€é©åŒ–
        if "vercel.json" in current_config.get("files", []):
            optimized["vercel_config"] = {"version": 2}
            optimizations.append("vercel.jsonç°¡ç´ åŒ–")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šè¿½åŠ 
        optimized["cache_headers"] = {
            "Cache-Control": "public, max-age=3600",
            "X-Optimized-By": "Vercel-Gemini-Integration"
        }
        optimizations.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ˜ãƒƒãƒ€ãƒ¼æœ€é©åŒ–")
        
        optimized["optimizations"] = optimizations
        return optimized
    
    def record_user_satisfaction(self, deployment_id: str, satisfaction_score: int, feedback: str = ""):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ã‚’è¨˜éŒ²"""
        if os.path.exists(self.user_satisfaction_file):
            with open(self.user_satisfaction_file, 'r', encoding='utf-8') as f:
                satisfaction_data = json.load(f)
        else:
            satisfaction_data = {"records": [], "average_score": 0}
        
        record = {
            "deployment_id": deployment_id,
            "timestamp": datetime.now().isoformat(),
            "score": satisfaction_score,  # 1-5
            "feedback": feedback
        }
        
        satisfaction_data["records"].append(record)
        
        # å¹³å‡ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = [r["score"] for r in satisfaction_data["records"]]
        satisfaction_data["average_score"] = sum(scores) / len(scores)
        
        with open(self.user_satisfaction_file, 'w', encoding='utf-8') as f:
            json.dump(satisfaction_data, f, ensure_ascii=False, indent=2)
        
        # AIå­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        self.ai_insights["user_preferences"][deployment_id] = {
            "score": satisfaction_score,
            "feedback": feedback
        }
        self._save_ai_insights()
    
    def generate_deployment_report(self) -> str:
        """çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = ["# Vercel Ã— Gemini çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆ\n"]
        report.append(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # æˆåŠŸç‡çµ±è¨ˆ
        if self.ai_insights["deployment_patterns"]:
            recent_patterns = self.ai_insights["deployment_patterns"][-10:]
            success_predictions = [p.get("ai_analysis", {}).get("success_probability", 0) 
                                 for p in recent_patterns 
                                 if "ai_analysis" in p]
            if success_predictions:
                avg_prediction = sum(success_predictions) / len(success_predictions)
                report.append(f"## ğŸ“Š AIäºˆæ¸¬æˆåŠŸç‡\n")
                report.append(f"- **å¹³å‡äºˆæ¸¬æˆåŠŸç‡**: {avg_prediction:.1f}%")
                report.append(f"- **åˆ†ææ¸ˆã¿ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ**: {len(success_predictions)}ä»¶\n")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦
        if os.path.exists(self.user_satisfaction_file):
            with open(self.user_satisfaction_file, 'r', encoding='utf-8') as f:
                satisfaction = json.load(f)
                if satisfaction.get("average_score"):
                    report.append(f"## ğŸ˜Š ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦\n")
                    report.append(f"- **å¹³å‡ã‚¹ã‚³ã‚¢**: {satisfaction['average_score']:.1f}/5.0")
                    report.append(f"- **è©•ä¾¡ä»¶æ•°**: {len(satisfaction.get('records', []))}ä»¶\n")
        
        # æœ€é©åŒ–ææ¡ˆ
        report.append("## ğŸš€ AIæœ€é©åŒ–ææ¡ˆ\n")
        if self.ai_insights.get("optimization_history"):
            recent_opts = self.ai_insights["optimization_history"][-3:]
            for opt in recent_opts:
                report.append(f"- {opt.get('suggestion', 'N/A')}")
        else:
            report.append("- é™çš„HTMLã‚µã‚¤ãƒˆã¸ã®ç§»è¡Œã‚’æ¨å¥¨")
            report.append("- vercel.jsonæœ€å°æ§‹æˆã®ç¶­æŒ")
            report.append("- å®šæœŸçš„ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿæ–½")
        
        # æˆåŠŸè¦å› 
        report.append("\n## âœ… æˆåŠŸè¦å› åˆ†æ\n")
        success_factors = self.deployment_manager.success_patterns.get("metadata", {}).get("common_issues", [])
        if success_factors:
            for factor in success_factors:
                report.append(f"- **{factor['issue']}**: {factor['solution']}")
        
        return "\n".join(report)
    
    async def intelligent_deploy(self, auto_optimize: bool = True) -> Dict:
        """AIæ”¯æ´ã«ã‚ˆã‚‹çŸ¥çš„ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ"""
        print("ğŸ¤– AIæ”¯æ´ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚’é–‹å§‹...")
        
        # ç¾åœ¨ã®æ§‹æˆã‚’åˆ†æ
        current_config = {
            "type": "static_html" if os.path.exists("public/index.html") else "unknown",
            "files": ["public/index.html", "vercel.json"] if os.path.exists("public/index.html") else []
        }
        
        # æˆåŠŸç‡äºˆæ¸¬
        success_rate, factors = self.predict_deployment_success(current_config)
        print(f"\nğŸ“Š äºˆæ¸¬æˆåŠŸç‡: {success_rate}%")
        for factor in factors:
            print(f"  - {factor}")
        
        # AIåˆ†æï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if self.model:
            print("\nğŸ” Gemini AIã«ã‚ˆã‚‹è©³ç´°åˆ†æä¸­...")
            ai_analysis = await self.analyze_deployment_with_ai(current_config)
            if ai_analysis.get("ai_available", False):
                print(f"  - AIæ¨å¥¨äº‹é …: {ai_analysis.get('recommendations', 'ãªã—')}")
        
        # æœ€é©åŒ–å®Ÿè¡Œ
        if auto_optimize and success_rate < 80:
            print("\nğŸ”§ æ§‹æˆã‚’æœ€é©åŒ–ä¸­...")
            optimized_config = self.optimize_deployment_config(current_config)
            for opt in optimized_config.get("optimizations", []):
                print(f"  - {opt}")
            
            # æœ€é©åŒ–ã‚’é©ç”¨
            if "é™çš„HTMLã«å¤‰æ›" in str(optimized_config.get("optimizations", [])):
                self.fix_assistant.apply_fix("static_html")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        backup_path = self.deployment_manager.backup_current_deployment()
        print(f"\nğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
        
        # ãƒ‡ãƒ—ãƒ­ã‚¤å®Ÿè¡Œ
        try:
            from direct_vercel_deploy import deploy_to_vercel
            success = deploy_to_vercel()
            
            if success:
                # æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³è¨˜éŒ²
                pattern = self.deployment_manager.record_success_pattern(
                    deployment_type="static_html",
                    files_changed=["public/index.html", "vercel.json"],
                    config_used=current_config,
                    success_reason="AIæœ€é©åŒ–ã«ã‚ˆã‚‹ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæˆåŠŸ",
                    deploy_id=f"ai_deploy_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    url="https://study-research-final.vercel.app"
                )
                
                return {
                    "success": True,
                    "pattern_id": pattern["id"],
                    "success_rate": success_rate,
                    "optimizations": optimized_config.get("optimizations", [])
                }
            else:
                # å¤±æ•—æ™‚ã®è‡ªå‹•ä¿®å¾©
                print("\nâŒ ãƒ‡ãƒ—ãƒ­ã‚¤å¤±æ•— - è‡ªå‹•ä¿®å¾©ã‚’é–‹å§‹...")
                self.fix_assistant.apply_fix("static_html")
                return {
                    "success": False,
                    "auto_fixed": True,
                    "message": "è‡ªå‹•ä¿®å¾©ã‚’å®Ÿè¡Œã—ã¾ã—ãŸ"
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "backup_available": backup_path
            }

# ä¾¿åˆ©ãªé–¢æ•°
async def smart_deploy():
    """ã‚¹ãƒãƒ¼ãƒˆãƒ‡ãƒ—ãƒ­ã‚¤å®Ÿè¡Œ"""
    integration = VercelGeminiIntegration()
    result = await integration.intelligent_deploy(auto_optimize=True)
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ã‚’å°‹ã­ã‚‹
    if result.get("success"):
        print("\nğŸ“ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®æº€è¶³åº¦ã‚’æ•™ãˆã¦ãã ã•ã„ (1-5):")
        try:
            score = int(input("ã‚¹ã‚³ã‚¢: "))
            feedback = input("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆä»»æ„ï¼‰: ")
            integration.record_user_satisfaction(
                deployment_id=result.get("pattern_id", "unknown"),
                satisfaction_score=score,
                feedback=feedback
            )
            print("âœ… ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²ã—ã¾ã—ãŸ")
        except:
            pass
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\n" + integration.generate_deployment_report())
    
    return result

if __name__ == "__main__":
    import asyncio
    
    print("ğŸš€ Vercel Ã— Gemini çµ±åˆã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    # éåŒæœŸå®Ÿè¡Œ
    asyncio.run(smart_deploy())