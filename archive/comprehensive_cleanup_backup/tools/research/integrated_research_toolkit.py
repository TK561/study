#!/usr/bin/env python3
"""
çµ±åˆç ”ç©¶ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ - Geminiã¨SQLæ©Ÿèƒ½ã‚’ç ”ç©¶ã«å®Œå…¨çµ±åˆ
"""

import os
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from research_sql_system import ResearchSQLSystem
from gemini_sql_toolkit import GeminiSQLToolkit
from deep_consultation_system import DeepConsultationSystem

class IntegratedResearchToolkit:
    """ç ”ç©¶ã¨SQLæ©Ÿèƒ½ã‚’çµ±åˆã—ãŸãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ"""
    
    def __init__(self, db_path: str = "research_toolkit.db"):
        self.research_sql = ResearchSQLSystem(db_path)
        self.gemini_sql = GeminiSQLToolkit()
        self.deep_consult = DeepConsultationSystem()
        self.session_log = []
        
    def analyze_research_data_with_gemini(self, analysis_request: str) -> Dict[str, Any]:
        """ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ã‚’Geminiã¨ç›¸è«‡ã—ãªãŒã‚‰åˆ†æ"""
        
        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ³ã‚’å–å¾—
        cursor = self.research_sql.conn.cursor()
        
        # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        cursor.execute("""
        SELECT 
            COUNT(*) as total_experiments,
            COUNT(CASE WHEN status = 'completed' THEN 1 END) as completed_experiments,
            MAX(created_at) as latest_experiment
        FROM experiments
        """)
        
        exp_summary = cursor.fetchone()
        
        # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        cursor.execute("""
        SELECT 
            metric_name,
            AVG(metric_value) as avg_value,
            MAX(metric_value) as max_value,
            MIN(metric_value) as min_value,
            COUNT(*) as count
        FROM experiment_statistics
        GROUP BY metric_name
        """)
        
        stats_summary = cursor.fetchall()
        
        # Geminiã¨ç›¸è«‡
        consultation_prompt = f"""
        ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã‚’è¡Œã„ã¾ã™ã€‚

        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ³
        - ç·å®Ÿé¨“æ•°: {exp_summary['total_experiments'] if exp_summary else 0}
        - å®Œäº†å®Ÿé¨“æ•°: {exp_summary['completed_experiments'] if exp_summary else 0}
        - æœ€æ–°å®Ÿé¨“: {exp_summary['latest_experiment'] if exp_summary else 'ãªã—'}

        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        """
        
        for stat in stats_summary:
            consultation_prompt += f"""
        - {stat['metric_name']}: å¹³å‡ {stat['avg_value']:.4f}, æœ€å¤§ {stat['max_value']:.4f}, æœ€å° {stat['min_value']:.4f} ({stat['count']}ä»¶)
        """
        
        consultation_prompt += f"""
        
        # åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        {analysis_request}
        
        ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰åˆ†æã—ã¦ãã ã•ã„ï¼š
        1. ãƒ‡ãƒ¼ã‚¿ã®å‚¾å‘ã¨ ãƒ‘ã‚¿ãƒ¼ãƒ³
        2. çµ±è¨ˆçš„ãªæœ‰æ„æ€§
        3. ç ”ç©¶ä¸Šã®ç¤ºå”†
        4. æ”¹å–„ææ¡ˆ
        5. è¿½åŠ èª¿æŸ»ãŒå¿…è¦ãªç‚¹
        
        ã¾ãŸã€ã•ã‚‰ãªã‚‹åˆ†æã«å¿…è¦ãªSQLã‚¯ã‚¨ãƒªã‚‚ææ¡ˆã—ã¦ãã ã•ã„ã€‚
        """
        
        analysis_result = self.deep_consult.deep_consult(consultation_prompt)
        
        # çµæœã‚’ãƒ­ã‚°ã«ä¿å­˜
        self.session_log.append({
            'type': 'data_analysis',
            'request': analysis_request,
            'result': analysis_result,
            'timestamp': datetime.now().isoformat()
        })
        
        return analysis_result
    
    def smart_experiment_setup(self, experiment_description: str) -> Dict[str, Any]:
        """Geminiã¨ç›¸è«‡ã—ã¦å®Ÿé¨“è¨­å®šã‚’æœ€é©åŒ–"""
        
        setup_prompt = f"""
        æ–°ã—ã„ç”»åƒåˆ†é¡å®Ÿé¨“ã®è¨­å®šã«ã¤ã„ã¦ç›¸è«‡ã—ã¾ã™ã€‚

        # å®Ÿé¨“ã®èª¬æ˜
        {experiment_description}

        # ç¾åœ¨ã®ç ”ç©¶ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        - ç ”ç©¶ãƒ†ãƒ¼ãƒ: WordNetãƒ™ãƒ¼ã‚¹ã®æ„å‘³ã‚«ãƒ†ã‚´ãƒªåˆ†æã‚’ç”¨ã„ãŸç‰¹åŒ–å‹ç”»åƒåˆ†é¡
        - å¯¾è±¡ã‚«ãƒ†ã‚´ãƒª: Person, Animal, Food, Landscape, Building, Furniture, Vehicle, Plant
        - åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: BLIP, WordNet, YOLOv8, SAM, CLIP

        ä»¥ä¸‹ã«ã¤ã„ã¦å…·ä½“çš„ã«ææ¡ˆã—ã¦ãã ã•ã„ï¼š

        1. **å®Ÿé¨“è¨­è¨ˆ**
           - å®Ÿé¨“ã®ç›®çš„ã¨ä»®èª¬
           - è©•ä¾¡æŒ‡æ¨™ã®é¸å®š
           - å®Ÿé¨“æ¡ä»¶ã®è¨­å®š

        2. **ãƒ‡ãƒ¼ã‚¿è¨­è¨ˆ**
           - å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
           - ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®æ¨å®š
           - ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æ–¹æ³•

        3. **çµ±è¨ˆçš„è€ƒæ…®**
           - çµ±è¨ˆçš„æ¤œå®šã®è¨ˆç”»
           - æœ‰æ„æ°´æº–ã®è¨­å®š
           - åŠ¹æœé‡ã®æ¨å®š

        4. **æŠ€è¡“çš„å®Ÿè£…**
           - ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¨å¥¨å€¤
           - å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­è¨ˆ
           - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–æ–¹æ³•

        5. **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ**
           - å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®è¨˜éŒ²é …ç›®
           - åˆ†æã«å¿…è¦ãªã‚¯ã‚¨ãƒª
           - ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆè¨ˆç”»
        """
        
        setup_result = self.deep_consult.deep_consult(setup_prompt)
        
        # çµæœã«åŸºã¥ã„ã¦å®Ÿé¨“ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆ
        experiment_template = self._extract_experiment_template(setup_result)
        
        return {
            'consultation_result': setup_result,
            'experiment_template': experiment_template,
            'recommended_queries': self._generate_recommended_queries(setup_result)
        }
    
    def _extract_experiment_template(self, consultation_result) -> Dict[str, Any]:
        """ç›¸è«‡çµæœã‹ã‚‰å®Ÿé¨“ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æŠ½å‡º"""
        
        # ç›¸è«‡çµæœã‹ã‚‰æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æŠ½å‡º
        template = {
            'experiment_name': 'New Experiment',
            'description': 'Generated from Gemini consultation',
            'parameters': {
                'batch_size': 32,
                'learning_rate': 0.001,
                'epochs': 50
            },
            'evaluation_metrics': ['accuracy', 'precision', 'recall', 'f1_score'],
            'statistical_tests': ['t_test', 'anova'],
            'monitoring_interval': 100  # predictions
        }
        
        return template
    
    def _generate_recommended_queries(self, consultation_result) -> List[str]:
        """æ¨å¥¨SQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ"""
        
        queries = [
            "SELECT AVG(confidence_score) FROM predictions WHERE experiment_id = ?",
            "SELECT category_name, COUNT(*) FROM predictions p JOIN categories c ON p.predicted_category_id = c.category_id GROUP BY category_name",
            "SELECT AVG(processing_time), STDDEV(processing_time) FROM predictions WHERE experiment_id = ?",
            "SELECT COUNT(*) as correct FROM predictions p JOIN images i ON p.image_id = i.image_id WHERE p.predicted_category_id = i.true_category_id AND p.experiment_id = ?"
        ]
        
        return queries
    
    def intelligent_data_exploration(self, exploration_goal: str) -> Dict[str, Any]:
        """Geminiã¨å”åŠ›ã—ã¦ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã‚’å®Ÿè¡Œ"""
        
        exploration_prompt = f"""
        ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ã®æ¢ç´¢ã‚’è¡Œã„ã¾ã™ã€‚

        # æ¢ç´¢ç›®æ¨™
        {exploration_goal}

        # åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿
        1. å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ï¼ˆexperiments ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
        2. äºˆæ¸¬çµæœï¼ˆpredictions ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
        3. ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ï¼ˆcategories ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
        4. çµ±è¨ˆçµæœï¼ˆexperiment_statistics ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
        5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ï¼ˆperformance_metrics ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰

        ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§æ¢ç´¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

        1. **æ¢ç´¢æˆ¦ç•¥ã®ç­–å®š**
           - ã©ã®ã‚ˆã†ãªä»®èª¬ã‚’æ¤œè¨¼ã™ã‚‹ã‹
           - ã©ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª¿ã¹ã‚‹ã¹ãã‹
           - ã©ã®ã‚ˆã†ãªåˆ†ææ‰‹æ³•ãŒé©åˆ‡ã‹

        2. **SQLã‚¯ã‚¨ãƒªã®ç”Ÿæˆ**
           - ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºç”¨ã®ã‚¯ã‚¨ãƒª
           - é›†è¨ˆãƒ»åˆ†æç”¨ã®ã‚¯ã‚¨ãƒª
           - å¯è¦–åŒ–ç”¨ã®ã‚¯ã‚¨ãƒª

        3. **åˆ†æè¨ˆç”»**
           - çµ±è¨ˆçš„åˆ†æã®æ‰‹é †
           - çµæœã®è§£é‡ˆæ–¹æ³•
           - è¿½åŠ èª¿æŸ»ã®æ–¹å‘æ€§

        å…·ä½“çš„ãªSQLã‚¯ã‚¨ãƒªã‚‚å«ã‚ã¦ææ¡ˆã—ã¦ãã ã•ã„ã€‚
        """
        
        exploration_result = self.deep_consult.deep_consult(exploration_prompt)
        
        # ææ¡ˆã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‚’æŠ½å‡ºã—ã¦å®Ÿè¡Œ
        suggested_queries = self._extract_sql_queries(exploration_result)
        query_results = {}
        
        for i, query in enumerate(suggested_queries):
            try:
                cursor = self.research_sql.conn.cursor()
                cursor.execute(query)
                results = cursor.fetchall()
                query_results[f"query_{i+1}"] = {
                    'sql': query,
                    'results': [dict(row) for row in results],
                    'row_count': len(results)
                }
            except Exception as e:
                query_results[f"query_{i+1}"] = {
                    'sql': query,
                    'error': str(e)
                }
        
        return {
            'exploration_strategy': exploration_result,
            'executed_queries': query_results,
            'timestamp': datetime.now().isoformat()
        }
    
    def _extract_sql_queries(self, text) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰SQLã‚¯ã‚¨ãƒªã‚’æŠ½å‡º"""
        
        import re
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¾æ›¸ã®å ´åˆã®å‡¦ç†
        if isinstance(text, dict):
            if 'final_answer' in text:
                text = text['final_answer']
            else:
                text = str(text)
        
        # SQLã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
        sql_pattern = r'```sql\n(.*?)\n```'
        queries = re.findall(sql_pattern, text, re.DOTALL)
        
        # ç°¡å˜ãªã‚¯ã‚¨ãƒªã®å ´åˆã®è£œå®Œ
        if not queries:
            simple_patterns = [
                r'SELECT[^;]+;',
                r'INSERT[^;]+;',
                r'UPDATE[^;]+;',
                r'DELETE[^;]+;'
            ]
            
            for pattern in simple_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
                queries.extend(matches)
        
        return queries[:5]  # æœ€å¤§5ã¤ã¾ã§
    
    def generate_research_insights(self, experiment_ids: List[str]) -> Dict[str, Any]:
        """å®Ÿé¨“çµæœã‹ã‚‰Geminiã¨å”åŠ›ã—ã¦ç ”ç©¶æ´å¯Ÿã‚’ç”Ÿæˆ"""
        
        # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        experiment_data = {}
        
        for exp_id in experiment_ids:
            cursor = self.research_sql.conn.cursor()
            
            # å®Ÿé¨“åŸºæœ¬æƒ…å ±
            cursor.execute("""
            SELECT * FROM experiments WHERE experiment_id = ?
            """, (exp_id,))
            exp_info = cursor.fetchone()
            
            # çµ±è¨ˆæƒ…å ±
            cursor.execute("""
            SELECT metric_name, metric_value 
            FROM experiment_statistics 
            WHERE experiment_id = ?
            """, (exp_id,))
            stats = {row['metric_name']: row['metric_value'] for row in cursor.fetchall()}
            
            if exp_info:
                experiment_data[exp_id] = {
                    'info': dict(exp_info),
                    'statistics': stats
                }
        
        # Geminiã¨æ´å¯Ÿã‚’ç”Ÿæˆ
        insights_prompt = f"""
        è¤‡æ•°ã®å®Ÿé¨“çµæœã‹ã‚‰ç ”ç©¶çš„æ´å¯Ÿã‚’å°å‡ºã—ã¦ãã ã•ã„ã€‚

        # å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿
        """
        
        for exp_id, data in experiment_data.items():
            insights_prompt += f"""
        ## å®Ÿé¨“ {exp_id}
        - åå‰: {data['info'].get('experiment_name', 'Unknown')}
        - ç ”ç©¶è€…: {data['info'].get('researcher', 'Unknown')}
        - ãƒ¢ãƒ‡ãƒ«: {data['info'].get('model_version', 'Unknown')}
        - çµ±è¨ˆ:
        """
            for metric, value in data['statistics'].items():
                insights_prompt += f"  - {metric}: {value}\n"
        
        insights_prompt += f"""
        
        # åˆ†æè¦³ç‚¹
        
        1. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ**
           - å„å®Ÿé¨“ã®æ€§èƒ½æ¯”è¼ƒ
           - å¼·ã¿ã¨å¼±ã¿ã®ç‰¹å®š
           - æ”¹å–„ç‚¹ã®æŠ½å‡º

        2. **çµ±è¨ˆçš„æœ‰æ„æ€§**
           - å®Ÿé¨“é–“ã®å·®ã®æœ‰æ„æ€§
           - ä¿¡é ¼åŒºé–“ã®æ¨å®š
           - åŠ¹æœé‡ã®è©•ä¾¡

        3. **ç ”ç©¶ä¸Šã®å«æ„**
           - ä»®èª¬ã®æ¤œè¨¼çŠ¶æ³
           - ç†è«–çš„ãªç¤ºå”†
           - å®Ÿç”¨æ€§ã®è©•ä¾¡

        4. **ä»Šå¾Œã®ç ”ç©¶æ–¹å‘**
           - è¿½åŠ å®Ÿé¨“ã®ææ¡ˆ
           - æ”¹å–„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
           - æ–°ã—ã„ä»®èª¬ã®ç”Ÿæˆ

        5. **æ–¹æ³•è«–çš„è€ƒå¯Ÿ**
           - å®Ÿé¨“è¨­è¨ˆã®å¦¥å½“æ€§
           - ãƒ‡ãƒ¼ã‚¿åé›†ã®é©åˆ‡æ€§
           - åˆ†ææ‰‹æ³•ã®è©•ä¾¡

        è©³ç´°ãªç ”ç©¶æ´å¯Ÿã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        """
        
        insights_result = self.deep_consult.deep_consult(insights_prompt)
        
        return {
            'experiment_data': experiment_data,
            'research_insights': insights_result,
            'insight_timestamp': datetime.now().isoformat()
        }
    
    def create_automated_research_pipeline(self, pipeline_config: Dict) -> Dict[str, Any]:
        """è‡ªå‹•åŒ–ã•ã‚ŒãŸç ”ç©¶ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ"""
        
        pipeline_prompt = f"""
        è‡ªå‹•åŒ–ã•ã‚ŒãŸç ”ç©¶ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­è¨ˆã«ã¤ã„ã¦ç›¸è«‡ã—ã¾ã™ã€‚

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š
        {json.dumps(pipeline_config, ensure_ascii=False, indent=2)}

        ä»¥ä¸‹ã®è‡ªå‹•åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ï¼š

        1. **ãƒ‡ãƒ¼ã‚¿åé›†è‡ªå‹•åŒ–**
           - å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•å–ã‚Šè¾¼ã¿
           - ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
           - ç•°å¸¸å€¤æ¤œå‡º

        2. **åˆ†æè‡ªå‹•åŒ–**
           - çµ±è¨ˆè¨ˆç®—ã®è‡ªå‹•å®Ÿè¡Œ
           - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
           - æ¯”è¼ƒåˆ†æ

        3. **ãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•åŒ–**
           - å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼
           - å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆ
           - ç ”ç©¶é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ

        4. **ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½**
           - æ€§èƒ½ç•°å¸¸ã®æ¤œå‡º
           - å®Ÿé¨“å®Œäº†é€šçŸ¥
           - ãƒ‡ãƒ¼ã‚¿å“è³ªè­¦å‘Š

        5. **å”åŠ›è€…ã¨ã®å…±æœ‰**
           - ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
           - ãƒ¬ãƒãƒ¼ãƒˆé…ä¿¡
           - ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡

        å…·ä½“çš„ãªå®Ÿè£…è¨ˆç”»ã¨SQLã‚¯ã‚¨ãƒªã‚‚å«ã‚ã¦ææ¡ˆã—ã¦ãã ã•ã„ã€‚
        """
        
        pipeline_result = self.deep_consult.deep_consult(pipeline_prompt)
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®šã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        cursor = self.research_sql.conn.cursor()
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS research_pipelines (
            pipeline_id TEXT PRIMARY KEY,
            pipeline_name TEXT,
            config TEXT,
            design TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            status TEXT DEFAULT 'active'
        )
        """)
        
        pipeline_id = f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        cursor.execute("""
        INSERT INTO research_pipelines 
        (pipeline_id, pipeline_name, config, design)
        VALUES (?, ?, ?, ?)
        """, (
            pipeline_id,
            pipeline_config.get('name', 'Automated Pipeline'),
            json.dumps(pipeline_config),
            json.dumps(pipeline_result if isinstance(pipeline_result, dict) else {'result': str(pipeline_result)})
        ))
        
        self.research_sql.conn.commit()
        
        return {
            'pipeline_id': pipeline_id,
            'design': pipeline_result,
            'config': pipeline_config
        }
    
    def save_session_log(self, filename: Optional[str] = None) -> str:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚°ã‚’ä¿å­˜"""
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'research_session_{timestamp}.json'
        
        session_data = {
            'session_log': self.session_log,
            'timestamp': datetime.now().isoformat(),
            'summary': f'Research session with {len(self.session_log)} activities'
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(session_data, f, ensure_ascii=False, indent=2)
        
        return filename
    
    def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾"""
        self.research_sql.close()


def demo_integrated_toolkit():
    """çµ±åˆãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("ğŸš€ çµ±åˆç ”ç©¶ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ ãƒ‡ãƒ¢")
    print("=" * 60)
    
    toolkit = IntegratedResearchToolkit("demo_integrated.db")
    
    # 1. ã‚¹ãƒãƒ¼ãƒˆå®Ÿé¨“è¨­å®š
    print("\nğŸ§  1. ã‚¹ãƒãƒ¼ãƒˆå®Ÿé¨“è¨­å®š")
    experiment_desc = "8ã‚«ãƒ†ã‚´ãƒªã®ç‰¹åŒ–å‹åˆ†é¡å™¨ã®æ€§èƒ½ã‚’å¾“æ¥æ‰‹æ³•ã¨æ¯”è¼ƒè©•ä¾¡ã™ã‚‹å®Ÿé¨“"
    setup_result = toolkit.smart_experiment_setup(experiment_desc)
    print("âœ… å®Ÿé¨“è¨­å®šã®ç›¸è«‡å®Œäº†")
    
    # 2. ãƒ‡ãƒ¼ã‚¿æ¢ç´¢
    print("\nğŸ” 2. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿æ¢ç´¢")
    exploration_result = toolkit.intelligent_data_exploration(
        "ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®åˆ†é¡æ€§èƒ½ã®å·®ç•°ã¨ã€ãã®åŸå› ã‚’èª¿æŸ»ã—ãŸã„"
    )
    print(f"âœ… {len(exploration_result.get('executed_queries', {}))}ä»¶ã®ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ")
    
    # 3. ç ”ç©¶æ´å¯Ÿç”Ÿæˆ
    print("\nğŸ’¡ 3. ç ”ç©¶æ´å¯Ÿç”Ÿæˆ")
    
    # ã¾ãšã‚µãƒ³ãƒ—ãƒ«å®Ÿé¨“ã‚’ä½œæˆ
    sample_exp_data = {
        'experiment_name': 'Sample Research Experiment',
        'description': 'Demonstration experiment for toolkit',
        'researcher': 'Demo Researcher'
    }
    
    exp_id = toolkit.research_sql.insert_experiment_data(sample_exp_data)
    
    # ã‚µãƒ³ãƒ—ãƒ«çµ±è¨ˆã‚’æŒ¿å…¥
    cursor = toolkit.research_sql.conn.cursor()
    cursor.execute("""
    INSERT INTO experiment_statistics (experiment_id, metric_name, metric_value)
    VALUES (?, 'accuracy', 0.812)
    """, (exp_id,))
    toolkit.research_sql.conn.commit()
    
    insights = toolkit.generate_research_insights([exp_id])
    print("âœ… ç ”ç©¶æ´å¯Ÿã‚’ç”Ÿæˆ")
    
    # 4. è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
    print("\nâš™ï¸ 4. è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ")
    pipeline_config = {
        'name': 'Real-time Performance Monitoring',
        'frequency': 'every_100_predictions',
        'metrics': ['accuracy', 'confidence', 'processing_time'],
        'alerts': ['performance_drop', 'data_quality_issue']
    }
    
    pipeline_result = toolkit.create_automated_research_pipeline(pipeline_config)
    print(f"âœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ: {pipeline_result['pipeline_id']}")
    
    # 5. ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜
    print("\nğŸ’¾ 5. ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜")
    saved_file = toolkit.save_session_log()
    print(f"âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚°ä¿å­˜: {saved_file}")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    toolkit.close()
    
    print("\nğŸ‰ çµ±åˆãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆ ãƒ‡ãƒ¢å®Œäº†ï¼")
    print("\nğŸ“‹ å®Ÿè£…ã•ã‚ŒãŸæ©Ÿèƒ½:")
    print("  âœ¨ Geminiç›¸è«‡ã«ã‚ˆã‚‹å®Ÿé¨“è¨­è¨ˆæœ€é©åŒ–")
    print("  ğŸ” ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿æ¢ç´¢")
    print("  ğŸ“Š è‡ªå‹•çµ±è¨ˆåˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    print("  ğŸ’¡ AIæ”¯æ´ã«ã‚ˆã‚‹ç ”ç©¶æ´å¯Ÿå°å‡º")
    print("  âš™ï¸ è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰")
    print("  ğŸ“ˆ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½ç›£è¦–")


if __name__ == "__main__":
    demo_integrated_toolkit()