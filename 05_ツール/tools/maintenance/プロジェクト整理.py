#!/usr/bin/env python3
"""
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•´ç†ã‚·ã‚¹ãƒ†ãƒ  - ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ•ã‚©ãƒ«ãƒ€ã®æœ€é©åŒ–
WordNetç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹é€ ã‚’æ•´ç†ã—ã€ä¸è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹
"""

import os
import shutil
import json
from datetime import datetime
from pathlib import Path
from collections import defaultdict

class ProjectCleanupOrganizer:
    def __init__(self):
        self.name = "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•´ç†ã‚·ã‚¹ãƒ†ãƒ "
        self.version = "1.0.0"
        self.root_path = Path("/mnt/c/Desktop/Research")
        self.backup_path = self.root_path / "cleanup_archive"
        self.backup_path.mkdir(exist_ok=True)
        
        # ä¿æŒã™ã¹ãé‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ•ã‚©ãƒ«ãƒ€
        self.keep_items = {
            "core_files": [
                "CLAUDE.md",
                "README.md",
                "vercel.json",
                "package.json",
                "requirements.txt",
                "WordNet-Based_Semantic_Image_Classification_Research_Presentation.pptx"
            ],
            "core_directories": [
                "system/implementations",  # çµ±åˆã•ã‚ŒãŸ5ã‚·ã‚¹ãƒ†ãƒ 
                "study",  # ç ”ç©¶å†…å®¹
                "sessions",  # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²
                "public",  # å…¬é–‹ãƒ•ã‚¡ã‚¤ãƒ«
                "config",  # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
                "logs"  # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
            ],
            "output_directories": [
                "system/implementations/output"  # ã‚·ã‚¹ãƒ†ãƒ å‡ºåŠ›
            ]
        }
        
        # å‰Šé™¤å¯¾è±¡ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾Œå‰Šé™¤ï¼‰
        self.cleanup_targets = {
            "redundant_backups": [
                "comprehensive_cleanup_backup",
                "system/pptx_analysis/implementation_templates"  # å®Ÿè£…æ¸ˆã¿ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
            ],
            "temporary_files": [
                "auto_cleanup.py",
                "auto_update_system.py",
                "cleanup_plan.py",
                "comprehensive_cleanup.py",
                "enhanced_pptx_analyzer.py",
                "gemini_integration.py",
                "pptx_reader.py",
                "session_save_protocol.py",
                "setup_auto_update.py",
                "simple_pptx_analyzer.py"
            ],
            "duplicate_analysis": [
                "system/pptx_analysis",  # åˆ†æå®Œäº†æ¸ˆã¿
                "ai_analysis"  # é‡è¤‡åˆ†æ
            ]
        }
        
        self.organization_plan = []
        
    def analyze_current_structure(self):
        """ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’åˆ†æ"""
        print("ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ åˆ†æä¸­...")
        
        analysis = {
            "total_files": 0,
            "total_directories": 0,
            "file_types": defaultdict(int),
            "large_files": [],
            "empty_directories": [],
            "structure": {}
        }
        
        for root, dirs, files in os.walk(self.root_path):
            rel_root = Path(root).relative_to(self.root_path)
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚«ã‚¦ãƒ³ãƒˆ
            analysis["total_directories"] += len(dirs)
            
            # ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒã‚§ãƒƒã‚¯
            if not dirs and not files:
                analysis["empty_directories"].append(str(rel_root))
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
            for file in files:
                file_path = Path(root) / file
                analysis["total_files"] += 1
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—
                extension = file_path.suffix.lower()
                analysis["file_types"][extension] += 1
                
                # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ« (10MBä»¥ä¸Š)
                try:
                    size = file_path.stat().st_size
                    if size > 10 * 1024 * 1024:  # 10MB
                        analysis["large_files"].append({
                            "path": str(file_path.relative_to(self.root_path)),
                            "size_mb": round(size / (1024 * 1024), 2)
                        })
                except:
                    pass
        
        return analysis
    
    def create_organization_plan(self):
        """æ•´ç†è¨ˆç”»ã‚’ä½œæˆ"""
        print("ğŸ“‹ æ•´ç†è¨ˆç”»ä½œæˆä¸­...")
        
        plan = {
            "keep_structure": {
                "core_research": {
                    "path": "research_core",
                    "contents": [
                        "study/",
                        "system/implementations/",
                        "WordNet-Based_Semantic_Image_Classification_Research_Presentation.pptx"
                    ]
                },
                "session_records": {
                    "path": "sessions",
                    "contents": [
                        "sessions/",
                        "logs/"
                    ]
                },
                "deployment": {
                    "path": "public",
                    "contents": [
                        "public/",
                        "vercel.json",
                        "package.json"
                    ]
                },
                "documentation": {
                    "path": "docs",
                    "contents": [
                        "CLAUDE.md",
                        "README.md",
                        "requirements.txt"
                    ]
                }
            },
            "archive_items": [],
            "delete_items": []
        }
        
        # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¯¾è±¡ã‚’ç‰¹å®š
        for category, items in self.cleanup_targets.items():
            for item in items:
                item_path = self.root_path / item
                if item_path.exists():
                    plan["archive_items"].append({
                        "path": item,
                        "category": category,
                        "type": "directory" if item_path.is_dir() else "file"
                    })
        
        self.organization_plan = plan
        return plan
    
    def backup_items(self, items):
        """ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        print("ğŸ’¾ é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸­...")
        
        backup_log = {
            "timestamp": datetime.now().isoformat(),
            "backed_up_items": []
        }
        
        for item in items:
            source_path = self.root_path / item["path"]
            backup_target = self.backup_path / item["path"]
            
            try:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                backup_target.parent.mkdir(parents=True, exist_ok=True)
                
                if source_path.is_dir():
                    if backup_target.exists():
                        shutil.rmtree(backup_target)
                    shutil.copytree(source_path, backup_target)
                else:
                    shutil.copy2(source_path, backup_target)
                
                backup_log["backed_up_items"].append({
                    "source": item["path"],
                    "backup": str(backup_target.relative_to(self.root_path)),
                    "type": item["type"],
                    "category": item["category"]
                })
                
                print(f"  âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {item['path']}")
                
            except Exception as e:
                print(f"  âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¤±æ•—: {item['path']} - {e}")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ­ã‚°ä¿å­˜
        log_path = self.backup_path / "backup_log.json"
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(backup_log, f, ensure_ascii=False, indent=2)
        
        return backup_log
    
    def execute_cleanup(self):
        """æ•´ç†å®Ÿè¡Œ"""
        print("ğŸ§¹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•´ç†å®Ÿè¡Œä¸­...")
        
        # 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
        if self.organization_plan["archive_items"]:
            backup_log = self.backup_items(self.organization_plan["archive_items"])
            print(f"ğŸ’¾ {len(backup_log['backed_up_items'])}ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
        
        # 2. ä¸è¦ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤
        deleted_items = []
        for item in self.organization_plan["archive_items"]:
            item_path = self.root_path / item["path"]
            try:
                if item_path.exists():
                    if item_path.is_dir():
                        shutil.rmtree(item_path)
                    else:
                        item_path.unlink()
                    deleted_items.append(item["path"])
                    print(f"  ğŸ—‘ï¸ å‰Šé™¤å®Œäº†: {item['path']}")
            except Exception as e:
                print(f"  âŒ å‰Šé™¤å¤±æ•—: {item['path']} - {e}")
        
        # 3. ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤
        empty_dirs_removed = self._remove_empty_directories()
        
        return {
            "deleted_items": deleted_items,
            "empty_dirs_removed": empty_dirs_removed,
            "backup_location": str(self.backup_path)
        }
    
    def _remove_empty_directories(self):
        """ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤"""
        removed_dirs = []
        
        # è¤‡æ•°å›å®Ÿè¡Œï¼ˆãƒã‚¹ãƒˆã—ãŸç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¯¾å¿œï¼‰
        for _ in range(3):
            for root, dirs, files in os.walk(self.root_path, topdown=False):
                for dir_name in dirs:
                    dir_path = Path(root) / dir_name
                    try:
                        if not any(dir_path.iterdir()):  # ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                            # é‡è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯å‰Šé™¤ã—ãªã„
                            rel_path = dir_path.relative_to(self.root_path)
                            if not any(str(rel_path).startswith(keep_dir) for keep_dir in self.keep_items["core_directories"]):
                                dir_path.rmdir()
                                removed_dirs.append(str(rel_path))
                    except:
                        pass
        
        return removed_dirs
    
    def generate_final_report(self, cleanup_result, initial_analysis):
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("ğŸ“„ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
        
        # æ•´ç†å¾Œã®åˆ†æ
        final_analysis = self.analyze_current_structure()
        
        report = f"""# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•´ç†å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“Š æ•´ç†å‰å¾Œã®æ¯”è¼ƒ

### æ•´ç†å‰
- ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {initial_analysis['total_files']}
- ç·ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {initial_analysis['total_directories']}
- ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {len(initial_analysis['empty_directories'])}

### æ•´ç†å¾Œ
- ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {final_analysis['total_files']}
- ç·ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {final_analysis['total_directories']}
- ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {len(final_analysis['empty_directories'])}

### å‰Šæ¸›åŠ¹æœ
- ãƒ•ã‚¡ã‚¤ãƒ«å‰Šæ¸›: {initial_analysis['total_files'] - final_analysis['total_files']}ä»¶
- ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šæ¸›: {initial_analysis['total_directories'] - final_analysis['total_directories']}ä»¶

## ğŸ—‘ï¸ å‰Šé™¤ãƒ»ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ 

### å‰Šé™¤ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ•ã‚©ãƒ«ãƒ€
"""
        
        for item in cleanup_result["deleted_items"]:
            report += f"- {item}\n"
        
        report += f"""
### å‰Šé™¤ã•ã‚ŒãŸç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ({len(cleanup_result["empty_dirs_removed"])}ä»¶)
"""
        
        for empty_dir in cleanup_result["empty_dirs_removed"]:
            report += f"- {empty_dir}\n"
        
        report += f"""

## ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±
- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å ´æ‰€: {cleanup_result["backup_location"]}
- å¾©å…ƒãŒå¿…è¦ãªå ´åˆã¯ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰å¾©å…ƒå¯èƒ½

## ğŸ“ æœ€çµ‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

### ä¿æŒã•ã‚Œã¦ã„ã‚‹é‡è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
"""
        
        for core_dir in self.keep_items["core_directories"]:
            dir_path = self.root_path / core_dir
            if dir_path.exists():
                report += f"- âœ… {core_dir}\n"
            else:
                report += f"- âŒ {core_dir} (å­˜åœ¨ã—ã¾ã›ã‚“)\n"
        
        report += f"""

### ä¿æŒã•ã‚Œã¦ã„ã‚‹é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«
"""
        
        for core_file in self.keep_items["core_files"]:
            file_path = self.root_path / core_file
            if file_path.exists():
                report += f"- âœ… {core_file}\n"
            else:
                report += f"- âŒ {core_file} (å­˜åœ¨ã—ã¾ã›ã‚“)\n"
        
        report += f"""

## ğŸ¯ æ•´ç†å®Œäº†å¾Œã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçŠ¶æ…‹

### ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ  (system/implementations/)
- WordNetéšå±¤å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ  âœ…
- å¤šå±¤ç‰©ä½“æ¤œå‡ºAPI âœ…
- å‹•çš„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠã‚¨ãƒ³ã‚¸ãƒ³ âœ…
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”»åƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  âœ…
- è‡ªå‹•è©•ä¾¡ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ  âœ…
- çµ±åˆç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ  âœ…

### ç ”ç©¶å†…å®¹ (study/)
- 15ãƒ¶æœˆç ”ç©¶è¨˜éŒ² âœ…
- 87.1%ç²¾åº¦é”æˆè¨˜éŒ² âœ…
- Session 13æº–å‚™è³‡æ–™ âœ…

### ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ² (sessions/)
- ç ”ç©¶æˆ¦ç•¥è¨˜éŒ² âœ…
- æ—¥æ¬¡ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚° âœ…

### ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ (public/)
- HTMLã‚µã‚¤ãƒˆ âœ…
- Vercelè¨­å®š âœ…

## ğŸ“ æ•´ç†å®Œäº†
- å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- æ•´ç†ã‚·ã‚¹ãƒ†ãƒ : {self.name} v{self.version}
- æ•´ç†çµæœ: æˆåŠŸ âœ…

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒæœ€é©åŒ–ã•ã‚Œã¾ã—ãŸã€‚ç ”ç©¶ç¶™ç¶šã¨Session 13æº–å‚™ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚
"""
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = self.root_path / "PROJECT_CLEANUP_REPORT.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return str(report_path)

def main():
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•´ç†å®Ÿè¡Œ"""
    print("ğŸ§¹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•´ç†ã‚·ã‚¹ãƒ†ãƒ  èµ·å‹•")
    print("=" * 60)
    print("ğŸ“ WordNetç ”ç©¶ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹é€ æœ€é©åŒ–ã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 60)
    
    organizer = ProjectCleanupOrganizer()
    
    # 1. ç¾çŠ¶åˆ†æ
    print("\nğŸ“Š STEP 1: ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ åˆ†æ")
    initial_analysis = organizer.analyze_current_structure()
    print(f"  ğŸ“ ç·ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {initial_analysis['total_directories']}")
    print(f"  ğŸ“„ ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {initial_analysis['total_files']}")
    print(f"  ğŸ—‚ï¸ ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {len(initial_analysis['empty_directories'])}ä»¶")
    
    if initial_analysis['large_files']:
        print(f"  ğŸ“¦ å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«: {len(initial_analysis['large_files'])}ä»¶")
        for large_file in initial_analysis['large_files'][:3]:  # ä¸Šä½3ä»¶è¡¨ç¤º
            print(f"    - {large_file['path']} ({large_file['size_mb']}MB)")
    
    # 2. æ•´ç†è¨ˆç”»ä½œæˆ
    print(f"\nğŸ“‹ STEP 2: æ•´ç†è¨ˆç”»ä½œæˆ")
    plan = organizer.create_organization_plan()
    print(f"  ğŸ—‘ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å¯¾è±¡: {len(plan['archive_items'])}ä»¶")
    
    # 3. æ•´ç†å®Ÿè¡Œ
    print(f"\nğŸ§¹ STEP 3: æ•´ç†å®Ÿè¡Œ")
    cleanup_result = organizer.execute_cleanup()
    print(f"  âœ… å‰Šé™¤å®Œäº†: {len(cleanup_result['deleted_items'])}ä»¶")
    print(f"  ğŸ—‚ï¸ ç©ºãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤: {len(cleanup_result['empty_dirs_removed'])}ä»¶")
    
    # 4. æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\nğŸ“„ STEP 4: æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    report_path = organizer.generate_final_report(cleanup_result, initial_analysis)
    print(f"  ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_path}")
    
    # å®Œäº†ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ğŸ‰ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•´ç†å®Œäº†")
    print("=" * 60)
    print("ğŸ“Š æ•´ç†çµæœ:")
    print(f"  ğŸ—‘ï¸ å‰Šé™¤ã‚¢ã‚¤ãƒ†ãƒ : {len(cleanup_result['deleted_items'])}ä»¶")
    print(f"  ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å ´æ‰€: {cleanup_result['backup_location']}")
    print(f"  ğŸ“‹ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    print("\nğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçŠ¶æ…‹:")
    print("  âœ… ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ : çµ±åˆå®Œäº† (5ã‚·ã‚¹ãƒ†ãƒ )")
    print("  âœ… ç ”ç©¶å†…å®¹: 87.1%ç²¾åº¦è¨˜éŒ²ä¿æŒ")
    print("  âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²: Session 13æº–å‚™å®Œäº†")
    print("  âœ… æ§‹é€ æœ€é©åŒ–: å®Œäº†")
    print("\nğŸš€ Session 13 (2025å¹´6æœˆ26æ—¥) æº–å‚™å®Œäº†")

if __name__ == "__main__":
    main()