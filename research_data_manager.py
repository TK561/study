#!/usr/bin/env python3
"""
ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ - ç”»åƒåˆ†é¡ç ”ç©¶å°‚ç”¨ã®ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
Geminiãªã—ã®ç´”ç²‹ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ©Ÿèƒ½ã®ã¿
"""

import os
import json
import sqlite3
import csv
from datetime import datetime
from typing import Dict, List, Optional, Any
from simple_research_sql import SimpleResearchSQL

class ResearchDataManager:
    """ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®çµ±åˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, db_path: str = "research_data.db"):
        self.sql = SimpleResearchSQL(db_path)
        self.import_log = []
        
    def import_experiment_from_python(self, experiment_data: Dict[str, Any]) -> str:
        """Pythonã®å®Ÿé¨“çµæœã‹ã‚‰ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        
        # å®Ÿé¨“ä½œæˆ
        exp_id = self.sql.create_experiment(
            name=experiment_data.get('name', 'æœªå‘½åå®Ÿé¨“'),
            description=experiment_data.get('description', ''),
            researcher=experiment_data.get('researcher', ''),
            model_type=experiment_data.get('model_type', ''),
            dataset_name=experiment_data.get('dataset_name', '')
        )
        
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
        if 'images' in experiment_data:
            self.sql.add_images(experiment_data['images'])
            self.import_log.append(f"ç”»åƒãƒ‡ãƒ¼ã‚¿ {len(experiment_data['images'])}ä»¶ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
        
        # äºˆæ¸¬çµæœãŒã‚ã‚‹å ´åˆ
        if 'predictions' in experiment_data:
            self.sql.add_prediction_batch(exp_id, experiment_data['predictions'])
            self.import_log.append(f"äºˆæ¸¬çµæœ {len(experiment_data['predictions'])}ä»¶ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
        
        self.import_log.append(f"å®Ÿé¨“ {exp_id} ã‚’ä½œæˆ")
        
        return exp_id
    
    def import_from_csv(self, predictions_csv: str, images_csv: Optional[str] = None) -> str:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        
        # CSVã‹ã‚‰äºˆæ¸¬çµæœã‚’èª­ã¿è¾¼ã¿
        predictions = []
        experiment_name = f"CSV_Import_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        with open(predictions_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                predictions.append({
                    'image_id': row['image_id'],
                    'predicted_category': int(row['predicted_category']),
                    'confidence': float(row['confidence']),
                    'processing_time': float(row.get('processing_time', 0.0))
                })
        
        # å®Ÿé¨“ä½œæˆ
        exp_id = self.sql.create_experiment(
            name=experiment_name,
            description=f"CSVã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ: {predictions_csv}",
            researcher="CSV Import"
        )
        
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
        if images_csv and os.path.exists(images_csv):
            images = []
            with open(images_csv, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    images.append({
                        'image_id': row['image_id'],
                        'path': row['path'],
                        'true_category': int(row['true_category'])
                    })
            
            self.sql.add_images(images)
            self.import_log.append(f"ç”»åƒãƒ‡ãƒ¼ã‚¿ {len(images)}ä»¶ã‚’CSVã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
        
        # äºˆæ¸¬çµæœã‚’è¿½åŠ 
        self.sql.add_prediction_batch(exp_id, predictions)
        self.import_log.append(f"äºˆæ¸¬çµæœ {len(predictions)}ä»¶ã‚’CSVã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
        
        return exp_id
    
    def create_wordnet_experiment(self, results_data: Dict[str, Any]) -> str:
        """WordNetç”»åƒåˆ†é¡ç ”ç©¶ã®çµæœã‚’ç™»éŒ²"""
        
        # ç ”ç©¶ç‰¹åŒ–ã®å®Ÿé¨“ã‚’ä½œæˆ
        exp_id = self.sql.create_experiment(
            name=f"WordNetç‰¹åŒ–å®Ÿé¨“_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            description="WordNetãƒ™ãƒ¼ã‚¹ã®æ„å‘³ã‚«ãƒ†ã‚´ãƒªåˆ†æã‚’ç”¨ã„ãŸç‰¹åŒ–å‹ç”»åƒåˆ†é¡",
            researcher=results_data.get('researcher', 'WordNetç ”ç©¶ãƒãƒ¼ãƒ '),
            model_type="BLIP+WordNet+CLIP",
            dataset_name="8ã‚«ãƒ†ã‚´ãƒªç‰¹åŒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
        )
        
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›
        if 'test_cases' in results_data:
            images = []
            predictions = []
            
            for i, test_case in enumerate(results_data['test_cases']):
                image_id = f"wordnet_test_{i:03d}"
                
                # ã‚«ãƒ†ã‚´ãƒªåã‚’IDã«å¤‰æ›
                category_mapping = {
                    'Person': 1, 'Animal': 2, 'Food': 3, 'Landscape': 4,
                    'Building': 5, 'Furniture': 6, 'Vehicle': 7, 'Plant': 8
                }
                
                true_category = category_mapping.get(test_case.get('wordnet_category'), 1)
                predicted_category = category_mapping.get(test_case.get('predicted_category'), 1)
                
                images.append({
                    'image_id': image_id,
                    'path': test_case.get('image_path', f'/wordnet/test_{i:03d}.jpg'),
                    'true_category': true_category
                })
                
                predictions.append({
                    'image_id': image_id,
                    'predicted_category': predicted_category,
                    'confidence': test_case.get('confidence', 0.0),
                    'processing_time': test_case.get('processing_time', 0.0)
                })
            
            self.sql.add_images(images)
            self.sql.add_prediction_batch(exp_id, predictions)
            
            self.import_log.append(f"WordNetãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {len(images)}ä»¶ã‚’ç™»éŒ²")
        
        # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ç™»éŒ²
        if 'overall_accuracy' in results_data:
            cursor = self.sql.conn.cursor()
            cursor.execute("""
            INSERT INTO experiment_stats (experiment_id, metric_name, value)
            VALUES (?, 'accuracy', ?)
            """, (exp_id, results_data['overall_accuracy']))
            
            self.sql.conn.commit()
            self.import_log.append(f"å…¨ä½“ç²¾åº¦ {results_data['overall_accuracy']:.4f} ã‚’ç™»éŒ²")
        
        return exp_id
    
    def bulk_import_experiments(self, experiments_dir: str) -> List[str]:
        """è¤‡æ•°ã®å®Ÿé¨“çµæœã‚’ä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        
        imported_experiments = []
        
        for filename in os.listdir(experiments_dir):
            if filename.endswith('.json'):
                filepath = os.path.join(experiments_dir, filename)
                
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        experiment_data = json.load(f)
                    
                    exp_id = self.import_experiment_from_python(experiment_data)
                    imported_experiments.append(exp_id)
                    self.import_log.append(f"ãƒ•ã‚¡ã‚¤ãƒ« {filename} ã‹ã‚‰å®Ÿé¨“ {exp_id} ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
                    
                except Exception as e:
                    self.import_log.append(f"ãƒ•ã‚¡ã‚¤ãƒ« {filename} ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {str(e)}")
        
        return imported_experiments
    
    def export_for_analysis(self, experiment_ids: List[str], output_dir: str = "analysis_export") -> Dict[str, str]:
        """åˆ†æç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        exported_files = {}
        
        for exp_id in experiment_ids:
            # JSONå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            json_file = self.sql.export_experiment_data(exp_id)
            
            # CSVå½¢å¼ã§ã‚‚å‡ºåŠ›
            cursor = self.sql.conn.cursor()
            
            # äºˆæ¸¬çµæœCSV
            cursor.execute("""
            SELECT 
                p.image_id,
                p.predicted_category,
                p.confidence,
                p.processing_time,
                i.true_category,
                c1.name as predicted_category_name,
                c2.name as true_category_name
            FROM predictions p
            JOIN images i ON p.image_id = i.image_id
            JOIN categories c1 ON p.predicted_category = c1.category_id
            JOIN categories c2 ON i.true_category = c2.category_id
            WHERE p.experiment_id = ?
            """, (exp_id,))
            
            results = cursor.fetchall()
            
            csv_filename = os.path.join(output_dir, f"{exp_id}_predictions.csv")
            
            with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
                if results:
                    writer = csv.DictWriter(f, fieldnames=results[0].keys())
                    writer.writeheader()
                    for row in results:
                        writer.writerow(dict(row))
            
            exported_files[exp_id] = {
                'json': json_file,
                'csv': csv_filename
            }
        
        self.import_log.append(f"{len(experiment_ids)}å€‹ã®å®Ÿé¨“ã‚’ {output_dir} ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        return exported_files
    
    def create_analysis_summary(self, experiment_ids: List[str]) -> Dict[str, Any]:
        """è¤‡æ•°å®Ÿé¨“ã®åˆ†æã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ"""
        
        summaries = []
        for exp_id in experiment_ids:
            summary = self.sql.get_experiment_summary(exp_id)
            if summary:
                summaries.append(summary)
        
        if not summaries:
            return {}
        
        # çµ±è¨ˆçš„ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—
        accuracies = [s['accuracy'] for s in summaries if s['accuracy'] > 0]
        confidences = [s['avg_confidence'] for s in summaries if s['avg_confidence'] > 0]
        processing_times = [s['avg_processing_time'] for s in summaries if s['avg_processing_time'] > 0]
        
        analysis_summary = {
            'total_experiments': len(summaries),
            'accuracy_stats': {
                'mean': sum(accuracies) / len(accuracies) if accuracies else 0,
                'max': max(accuracies) if accuracies else 0,
                'min': min(accuracies) if accuracies else 0,
                'count': len(accuracies)
            },
            'confidence_stats': {
                'mean': sum(confidences) / len(confidences) if confidences else 0,
                'max': max(confidences) if confidences else 0,
                'min': min(confidences) if confidences else 0
            },
            'processing_time_stats': {
                'mean': sum(processing_times) / len(processing_times) if processing_times else 0,
                'max': max(processing_times) if processing_times else 0,
                'min': min(processing_times) if processing_times else 0
            },
            'experiments': summaries,
            'best_experiment': max(summaries, key=lambda x: x['accuracy']) if summaries else None,
            'analysis_date': datetime.now().isoformat()
        }
        
        return analysis_summary
    
    def generate_comparative_report(self, experiment_ids: List[str]) -> str:
        """æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        analysis = self.create_analysis_summary(experiment_ids)
        
        if not analysis:
            return "åˆ†æã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        report = f"""
# ç ”ç©¶å®Ÿé¨“æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ

## åˆ†ææ¦‚è¦
- **åˆ†æå®Ÿé¨“æ•°**: {analysis['total_experiments']}
- **åˆ†ææ—¥æ™‚**: {analysis['analysis_date']}

## ç²¾åº¦çµ±è¨ˆ
- **å¹³å‡ç²¾åº¦**: {analysis['accuracy_stats']['mean']:.4f}
- **æœ€é«˜ç²¾åº¦**: {analysis['accuracy_stats']['max']:.4f}
- **æœ€ä½ç²¾åº¦**: {analysis['accuracy_stats']['min']:.4f}

## ç¢ºä¿¡åº¦çµ±è¨ˆ
- **å¹³å‡ç¢ºä¿¡åº¦**: {analysis['confidence_stats']['mean']:.4f}
- **æœ€é«˜ç¢ºä¿¡åº¦**: {analysis['confidence_stats']['max']:.4f}
- **æœ€ä½ç¢ºä¿¡åº¦**: {analysis['confidence_stats']['min']:.4f}

## å‡¦ç†æ™‚é–“çµ±è¨ˆ
- **å¹³å‡å‡¦ç†æ™‚é–“**: {analysis['processing_time_stats']['mean']:.4f}ç§’
- **æœ€çŸ­å‡¦ç†æ™‚é–“**: {analysis['processing_time_stats']['min']:.4f}ç§’
- **æœ€é•·å‡¦ç†æ™‚é–“**: {analysis['processing_time_stats']['max']:.4f}ç§’

## å®Ÿé¨“è©³ç´°

| å®Ÿé¨“ID | å®Ÿé¨“å | ç²¾åº¦ | ç¢ºä¿¡åº¦ | å‡¦ç†æ™‚é–“ | äºˆæ¸¬æ•° |
|--------|--------|------|--------|----------|--------|
"""
        
        for exp in analysis['experiments']:
            report += f"| {exp['experiment_id']} | {exp['name']} | {exp['accuracy']:.4f} | {exp['avg_confidence']:.4f} | {exp['avg_processing_time']:.4f} | {exp['total_predictions']} |\n"
        
        if analysis['best_experiment']:
            best = analysis['best_experiment']
            report += f"""
## æœ€é«˜æ€§èƒ½å®Ÿé¨“
- **å®Ÿé¨“ID**: {best['experiment_id']}
- **å®Ÿé¨“å**: {best['name']}
- **ç²¾åº¦**: {best['accuracy']:.4f}
- **ç ”ç©¶è€…**: {best['researcher']}
- **ãƒ¢ãƒ‡ãƒ«**: {best['model_type']}
"""
        
        report += f"""
---
ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚: {datetime.now().isoformat()}
"""
        
        return report
    
    def get_import_log(self) -> List[str]:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ­ã‚°ã‚’å–å¾—"""
        return self.import_log.copy()
    
    def clear_import_log(self):
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ­ã‚°ã‚’ã‚¯ãƒªã‚¢"""
        self.import_log.clear()
    
    def close(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾"""
        self.sql.close()


def demo_research_data_manager():
    """ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ãƒ‡ãƒ¢"""
    
    print("ğŸ“Š ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ ãƒ‡ãƒ¢")
    print("=" * 50)
    
    manager = ResearchDataManager("demo_research_manager.db")
    
    # 1. WordNetç ”ç©¶çµæœã®ç™»éŒ²
    print("\n1. WordNetç ”ç©¶çµæœã‚’ç™»éŒ²ä¸­...")
    
    wordnet_results = {
        'researcher': 'WordNetç ”ç©¶ãƒãƒ¼ãƒ ',
        'overall_accuracy': 0.8125,
        'test_cases': [
            {
                'image_path': '/wordnet/person_001.jpg',
                'wordnet_category': 'Person',
                'predicted_category': 'Person',
                'confidence': 0.95,
                'processing_time': 0.12
            },
            {
                'image_path': '/wordnet/animal_001.jpg',
                'wordnet_category': 'Animal',
                'predicted_category': 'Animal',
                'confidence': 0.88,
                'processing_time': 0.11
            },
            {
                'image_path': '/wordnet/food_001.jpg',
                'wordnet_category': 'Food',
                'predicted_category': 'Animal',  # èª¤åˆ†é¡
                'confidence': 0.76,
                'processing_time': 0.13
            },
            {
                'image_path': '/wordnet/landscape_001.jpg',
                'wordnet_category': 'Landscape',
                'predicted_category': 'Landscape',
                'confidence': 0.92,
                'processing_time': 0.10
            }
        ]
    }
    
    wordnet_exp_id = manager.create_wordnet_experiment(wordnet_results)
    print(f"âœ… WordNetå®Ÿé¨“ã‚’ç™»éŒ²: {wordnet_exp_id}")
    
    # 2. æ¯”è¼ƒå®Ÿé¨“ã®ä½œæˆ
    print("\n2. æ¯”è¼ƒå®Ÿé¨“ã‚’ä½œæˆä¸­...")
    
    comparison_data = {
        'name': 'å¾“æ¥æ‰‹æ³•æ¯”è¼ƒå®Ÿé¨“',
        'description': 'æ±ç”¨ãƒ¢ãƒ‡ãƒ«ã¨ã®æ€§èƒ½æ¯”è¼ƒ',
        'researcher': 'Comparison Team',
        'model_type': 'Generic CLIP',
        'images': [
            {'image_id': 'comp_001', 'path': '/comp/001.jpg', 'true_category': 1},
            {'image_id': 'comp_002', 'path': '/comp/002.jpg', 'true_category': 2},
            {'image_id': 'comp_003', 'path': '/comp/003.jpg', 'true_category': 3},
            {'image_id': 'comp_004', 'path': '/comp/004.jpg', 'true_category': 4}
        ],
        'predictions': [
            {'image_id': 'comp_001', 'predicted_category': 1, 'confidence': 0.82, 'processing_time': 0.15},
            {'image_id': 'comp_002', 'predicted_category': 3, 'confidence': 0.75, 'processing_time': 0.14},  # èª¤åˆ†é¡
            {'image_id': 'comp_003', 'predicted_category': 3, 'confidence': 0.89, 'processing_time': 0.16},
            {'image_id': 'comp_004', 'predicted_category': 4, 'confidence': 0.91, 'processing_time': 0.12}
        ]
    }
    
    comparison_exp_id = manager.import_experiment_from_python(comparison_data)
    print(f"âœ… æ¯”è¼ƒå®Ÿé¨“ã‚’ä½œæˆ: {comparison_exp_id}")
    
    # 3. æ¯”è¼ƒåˆ†æ
    print("\n3. æ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œä¸­...")
    
    experiment_ids = [wordnet_exp_id, comparison_exp_id]
    comparative_report = manager.generate_comparative_report(experiment_ids)
    
    print("ğŸ“„ æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ:")
    print(comparative_report[:800] + "...")
    
    # 4. ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    print("\n4. åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
    
    exported_files = manager.export_for_analysis(experiment_ids, "demo_export")
    
    for exp_id, files in exported_files.items():
        print(f"âœ… {exp_id}: JSON={files['json']}, CSV={files['csv']}")
    
    # 5. ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ­ã‚°ç¢ºèª
    print("\n5. ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ­ã‚°:")
    for log_entry in manager.get_import_log():
        print(f"  - {log_entry}")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    manager.close()
    
    print("\nğŸ‰ ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ ãƒ‡ãƒ¢å®Œäº†ï¼")
    
    print("\nğŸ“‹ æä¾›æ©Ÿèƒ½:")
    print("  ğŸ“¥ å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
    print("  ğŸ“Š è¤‡æ•°å®Ÿé¨“ã®æ¯”è¼ƒåˆ†æ")
    print("  ğŸ“ˆ çµ±è¨ˆçš„ã‚µãƒãƒªãƒ¼ç”Ÿæˆ")
    print("  ğŸ’¾ åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    print("  ğŸ“„ è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")


if __name__ == "__main__":
    demo_research_data_manager()