#!/usr/bin/env python3
"""
è‡ªå‹•è©•ä¾¡ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ  - å®Œå…¨å®Ÿè£…ç‰ˆ
ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è‡ªå‹•è©•ä¾¡ã¨æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import time
from datetime import datetime
from pathlib import Path
import random
from collections import defaultdict
import statistics

class AutoEvaluationBenchmark:
    def __init__(self):
        self.name = "è‡ªå‹•è©•ä¾¡ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ "
        self.version = "2.0.0"
        self.output_dir = Path("output/benchmarks")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.benchmark_history = []
        
        # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
        self.metrics = {
            "accuracy": {"name": "ç²¾åº¦", "unit": "", "higher_is_better": True},
            "precision": {"name": "é©åˆç‡", "unit": "", "higher_is_better": True},
            "recall": {"name": "å†ç¾ç‡", "unit": "", "higher_is_better": True},
            "f1_score": {"name": "F1ã‚¹ã‚³ã‚¢", "unit": "", "higher_is_better": True},
            "processing_time": {"name": "å‡¦ç†æ™‚é–“", "unit": "ms", "higher_is_better": False},
            "memory_usage": {"name": "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡", "unit": "MB", "higher_is_better": False},
            "throughput": {"name": "ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ", "unit": "fps", "higher_is_better": True}
        }
        
    def generate_confusion_matrix(self, num_classes=5):
        """æ··åŒè¡Œåˆ—ã‚’ç”Ÿæˆï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰"""
        matrix = [[0 for _ in range(num_classes)] for _ in range(num_classes)]
        
        # å¯¾è§’ç·šä¸Šã«é«˜ã„å€¤ã‚’é…ç½®ï¼ˆæ­£è§£ï¼‰
        for i in range(num_classes):
            matrix[i][i] = random.randint(7, 9)  # 7-9å€‹ã®æ­£è§£
            
            # éå¯¾è§’è¦ç´ ï¼ˆèª¤åˆ†é¡ï¼‰
            for j in range(num_classes):
                if i != j:
                    matrix[i][j] = random.randint(0, 2)  # 0-2å€‹ã®èª¤åˆ†é¡
        
        return matrix
    
    def calculate_metrics_from_confusion_matrix(self, confusion_matrix):
        """æ··åŒè¡Œåˆ—ã‹ã‚‰å„ç¨®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        num_classes = len(confusion_matrix)
        metrics = {}
        
        # å…¨ä½“ã®ç²¾åº¦
        total_correct = sum(confusion_matrix[i][i] for i in range(num_classes))
        total_samples = sum(sum(row) for row in confusion_matrix)
        metrics["accuracy"] = (total_correct / total_samples) if total_samples > 0 else 0
        
        # ã‚¯ãƒ©ã‚¹ã”ã¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        class_metrics = []
        for i in range(num_classes):
            tp = confusion_matrix[i][i]
            fp = sum(confusion_matrix[j][i] for j in range(num_classes)) - tp
            fn = sum(confusion_matrix[i]) - tp
            tn = total_samples - tp - fp - fn
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            class_metrics.append({
                "class_id": i,
                "precision": precision,
                "recall": recall,
                "f1_score": f1,
                "support": sum(confusion_matrix[i])
            })
        
        # ãƒã‚¯ãƒ­å¹³å‡
        metrics["precision"] = sum(m["precision"] for m in class_metrics) / len(class_metrics)
        metrics["recall"] = sum(m["recall"] for m in class_metrics) / len(class_metrics)
        metrics["f1_score"] = sum(m["f1_score"] for m in class_metrics) / len(class_metrics)
        
        return metrics, class_metrics
    
    def simulate_model_performance(self, model_name, dataset_name):
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰"""
        # ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§ã«åŸºã¥ããƒ™ãƒ¼ã‚¹æ€§èƒ½
        model_characteristics = {
            "resnet50": {"base_accuracy": 85, "speed": 50, "memory": 200},
            "efficientnet": {"base_accuracy": 87, "speed": 35, "memory": 150},
            "mobilenet": {"base_accuracy": 80, "speed": 20, "memory": 50},
            "custom_model": {"base_accuracy": 83, "speed": 45, "memory": 180},
            "ensemble": {"base_accuracy": 90, "speed": 80, "memory": 400}
        }
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰¹æ€§
        dataset_difficulty = {
            "easy": 1.1,
            "medium": 1.0,
            "hard": 0.9,
            "very_hard": 0.85
        }
        
        # ãƒ™ãƒ¼ã‚¹æ€§èƒ½å–å¾—
        if model_name in model_characteristics:
            base = model_characteristics[model_name]
        else:
            base = {"base_accuracy": 82, "speed": 40, "memory": 160}
        
        # ãƒ©ãƒ³ãƒ€ãƒ è¦ç´ ã‚’åŠ ãˆãŸæ€§èƒ½
        difficulty = dataset_difficulty.get(dataset_name, 1.0)
        
        performance = {
            "accuracy": min(1.0, (base["base_accuracy"] / 100) * difficulty + random.uniform(-0.05, 0.05)),
            "processing_time": base["speed"] + random.uniform(-10, 10),
            "memory_usage": base["memory"] + random.uniform(-20, 20),
            "throughput": 1000 / base["speed"] + random.uniform(-2, 2)
        }
        
        return performance
    
    def run_single_benchmark(self, model_config, dataset_config, num_samples=1000):
        """å˜ä¸€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œ"""
        start_time = time.time()
        
        # æ··åŒè¡Œåˆ—ç”Ÿæˆ
        confusion_matrix = self.generate_confusion_matrix(
            num_classes=dataset_config.get("num_classes", 5)
        )
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        metrics, class_metrics = self.calculate_metrics_from_confusion_matrix(confusion_matrix)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        performance = self.simulate_model_performance(
            model_config["name"],
            dataset_config.get("difficulty", "medium")
        )
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±åˆï¼ˆaccuracyã¯æ··åŒè¡Œåˆ—ã®å€¤ã‚’å„ªå…ˆï¼‰
        confusion_accuracy = metrics["accuracy"]
        metrics.update(performance)
        metrics["accuracy"] = confusion_accuracy  # æ··åŒè¡Œåˆ—ç”±æ¥ã®ç²¾åº¦ã‚’ä¿æŒ
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ™‚é–“
        benchmark_time = time.time() - start_time
        
        result = {
            "model": model_config,
            "dataset": dataset_config,
            "metrics": metrics,
            "class_metrics": class_metrics,
            "confusion_matrix": confusion_matrix,
            "num_samples": num_samples,
            "benchmark_time": round(benchmark_time, 3),
            "timestamp": datetime.now().isoformat()
        }
        
        return result
    
    def run_comparative_benchmark(self, models, datasets):
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("ğŸƒ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
        
        all_results = []
        comparison_matrix = defaultdict(lambda: defaultdict(dict))
        
        total_combinations = len(models) * len(datasets)
        current = 0
        
        for model in models:
            for dataset in datasets:
                current += 1
                print(f"  [{current}/{total_combinations}] {model['name']} on {dataset['name']}")
                
                # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
                result = self.run_single_benchmark(model, dataset)
                all_results.append(result)
                
                # æ¯”è¼ƒãƒãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ 
                for metric_name, metric_value in result["metrics"].items():
                    comparison_matrix[metric_name][model["name"]][dataset["name"]] = metric_value
        
        # ç·åˆåˆ†æ
        analysis = self.analyze_benchmark_results(all_results, comparison_matrix)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = {
            "benchmark_name": "æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯",
            "timestamp": datetime.now().isoformat(),
            "models": models,
            "datasets": datasets,
            "results": all_results,
            "comparison_matrix": dict(comparison_matrix),
            "analysis": analysis
        }
        
        # å±¥æ­´ã«è¿½åŠ 
        self.benchmark_history.append(report)
        
        return report
    
    def analyze_benchmark_results(self, results, comparison_matrix):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®åˆ†æ"""
        analysis = {
            "best_performers": {},
            "metric_statistics": {},
            "model_rankings": {},
            "recommendations": []
        }
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã”ã¨ã®çµ±è¨ˆ
        for metric_name, metric_info in self.metrics.items():
            if metric_name in comparison_matrix:
                all_values = []
                for model_results in comparison_matrix[metric_name].values():
                    all_values.extend(model_results.values())
                
                if all_values:
                    analysis["metric_statistics"][metric_name] = {
                        "mean": statistics.mean(all_values),
                        "std": statistics.stdev(all_values) if len(all_values) > 1 else 0,
                        "min": min(all_values),
                        "max": max(all_values)
                    }
                    
                    # ãƒ™ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ¼ç‰¹å®š
                    best_value = max(all_values) if metric_info["higher_is_better"] else min(all_values)
                    for model, datasets in comparison_matrix[metric_name].items():
                        for dataset, value in datasets.items():
                            if value == best_value:
                                analysis["best_performers"][metric_name] = {
                                    "model": model,
                                    "dataset": dataset,
                                    "value": value
                                }
                                break
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨ˆç®—
        model_scores = defaultdict(list)
        for result in results:
            model_name = result["model"]["name"]
            
            # æ­£è¦åŒ–ã‚¹ã‚³ã‚¢è¨ˆç®—
            for metric_name, metric_value in result["metrics"].items():
                if metric_name in self.metrics:
                    stats = analysis["metric_statistics"].get(metric_name, {})
                    if stats and stats["max"] != stats["min"]:
                        # 0-1ã«æ­£è¦åŒ–
                        if self.metrics[metric_name]["higher_is_better"]:
                            normalized = (metric_value - stats["min"]) / (stats["max"] - stats["min"])
                        else:
                            normalized = 1 - (metric_value - stats["min"]) / (stats["max"] - stats["min"])
                        model_scores[model_name].append(normalized)
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        for model_name, scores in model_scores.items():
            analysis["model_rankings"][model_name] = {
                "average_score": statistics.mean(scores) if scores else 0,
                "num_metrics": len(scores)
            }
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚½ãƒ¼ãƒˆ
        analysis["model_rankings"] = dict(
            sorted(analysis["model_rankings"].items(),
                   key=lambda x: x[1]["average_score"],
                   reverse=True)
        )
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        if analysis["model_rankings"]:
            best_model = list(analysis["model_rankings"].keys())[0]
            analysis["recommendations"].append(
                f"ç·åˆçš„ã«æœ€ã‚‚å„ªã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ {best_model} ã§ã™"
            )
        
        if "accuracy" in analysis["metric_statistics"]:
            avg_accuracy = analysis["metric_statistics"]["accuracy"]["mean"]
            if avg_accuracy < 80:
                analysis["recommendations"].append(
                    "å…¨ä½“çš„ãªç²¾åº¦ãŒä½ã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¦‹ç›´ã—ã‚’æ¨å¥¨ã—ã¾ã™"
                )
        
        if "processing_time" in analysis["best_performers"]:
            fastest = analysis["best_performers"]["processing_time"]
            analysis["recommendations"].append(
                f"æœ€é€Ÿå‡¦ç†ã¯ {fastest['model']} ã§ {fastest['value']:.1f}ms ã§ã™"
            )
        
        return analysis
    
    def generate_benchmark_report_html(self, report):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®HTMLç”Ÿæˆ"""
        html_content = f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ - {report['timestamp']}</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1, h2, h3 {{
            color: #333;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        th, td {{
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }}
        th {{
            background-color: #f8f9fa;
            font-weight: bold;
        }}
        tr:nth-child(even) {{
            background-color: #f8f9fa;
        }}
        .metric-value {{
            font-weight: bold;
            text-align: right;
        }}
        .best-value {{
            background-color: #d4edda;
            color: #155724;
        }}
        .recommendation {{
            background-color: #e7f3ff;
            border-left: 4px solid #007bff;
            padding: 15px;
            margin: 10px 0;
        }}
        .chart {{
            margin: 20px 0;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 8px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ† è‡ªå‹•è©•ä¾¡ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <p><strong>å®Ÿè¡Œæ—¥æ™‚:</strong> {datetime.fromisoformat(report['timestamp']).strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        
        <h2>ğŸ“Š æ¯”è¼ƒãƒãƒˆãƒªã‚¯ã‚¹</h2>
"""
        
        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
        for metric_name, metric_data in report["comparison_matrix"].items():
            if metric_name in self.metrics:
                metric_info = self.metrics[metric_name]
                html_content += f"""
        <h3>{metric_info['name']}{f" ({metric_info['unit']})" if metric_info['unit'] else ""}</h3>
        <table>
            <tr>
                <th>ãƒ¢ãƒ‡ãƒ«</th>
"""
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ—
                datasets = list(report["datasets"])
                for dataset in datasets:
                    html_content += f"<th>{dataset['name']}</th>"
                html_content += "</tr>"
                
                # å„ãƒ¢ãƒ‡ãƒ«ã®è¡Œ
                for model_name, dataset_results in metric_data.items():
                    html_content += f"<tr><td><strong>{model_name}</strong></td>"
                    for dataset in datasets:
                        value = dataset_results.get(dataset['name'], '-')
                        if isinstance(value, (int, float)):
                            # ãƒ™ã‚¹ãƒˆå€¤ãƒã‚§ãƒƒã‚¯
                            is_best = False
                            best_performer = report['analysis']['best_performers'].get(metric_name)
                            if best_performer and best_performer['model'] == model_name and best_performer['dataset'] == dataset['name']:
                                is_best = True
                            
                            cell_class = "metric-value best-value" if is_best else "metric-value"
                            html_content += f'<td class="{cell_class}">{value:.2f}</td>'
                        else:
                            html_content += f'<td class="metric-value">{value}</td>'
                    html_content += "</tr>"
                
                html_content += "</table>"
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        html_content += """
        <h2>ğŸ… ãƒ¢ãƒ‡ãƒ«ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°</h2>
        <table>
            <tr>
                <th>é †ä½</th>
                <th>ãƒ¢ãƒ‡ãƒ«</th>
                <th>ç·åˆã‚¹ã‚³ã‚¢</th>
            </tr>
"""
        
        for i, (model_name, score_info) in enumerate(report['analysis']['model_rankings'].items(), 1):
            html_content += f"""
            <tr>
                <td>{i}</td>
                <td><strong>{model_name}</strong></td>
                <td class="metric-value">{score_info['average_score']:.3f}</td>
            </tr>
"""
        
        html_content += """
        </table>
        
        <h2>ğŸ’¡ æ¨å¥¨äº‹é …</h2>
"""
        
        for recommendation in report['analysis']['recommendations']:
            html_content += f"""
        <div class="recommendation">
            {recommendation}
        </div>
"""
        
        html_content += """
    </div>
</body>
</html>"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = self.output_dir / f"benchmark_report_{timestamp}.html"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return str(report_path)
    
    def export_results_json(self, report):
        """çµæœã‚’JSONå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        json_path = self.output_dir / f"benchmark_results_{timestamp}.json"
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return str(json_path)

def main():
    """å®Ÿè¡Œä¾‹"""
    print("ğŸ“Š è‡ªå‹•è©•ä¾¡ãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ  èµ·å‹•")
    print("=" * 50)
    
    benchmark = AutoEvaluationBenchmark()
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«å®šç¾©
    models = [
        {"name": "resnet50", "version": "1.0", "type": "CNN"},
        {"name": "efficientnet", "version": "b0", "type": "CNN"},
        {"name": "mobilenet", "version": "v3", "type": "lightweight"},
        {"name": "custom_model", "version": "2.0", "type": "hybrid"},
        {"name": "ensemble", "version": "1.0", "type": "ensemble"}
    ]
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®šç¾©
    datasets = [
        {"name": "COCO_subset", "difficulty": "medium", "num_classes": 5},
        {"name": "ImageNet_subset", "difficulty": "hard", "num_classes": 10},
        {"name": "Custom_dataset", "difficulty": "easy", "num_classes": 3}
    ]
    
    # æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    print("\nğŸš€ æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹...")
    report = benchmark.run_comparative_benchmark(models, datasets)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    html_path = benchmark.generate_benchmark_report_html(report)
    print(f"\nğŸ“„ HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {html_path}")
    
    # JSON ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    json_path = benchmark.export_results_json(report)
    print(f"ğŸ’¾ JSONçµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: {json_path}")
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\nğŸ“ˆ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚µãƒãƒªãƒ¼:")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ•°: {len(models)}")
    print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {len(datasets)}")
    print(f"  ç·ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ•°: {len(models) * len(datasets)}")
    
    print("\nğŸ† ãƒˆãƒƒãƒ—3ãƒ¢ãƒ‡ãƒ«:")
    for i, (model_name, score_info) in enumerate(list(report['analysis']['model_rankings'].items())[:3], 1):
        print(f"  {i}. {model_name} (ã‚¹ã‚³ã‚¢: {score_info['average_score']:.3f})")
    
    print("\nâœ¨ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
    print(f"ğŸŒ ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º: file://{Path(html_path).absolute()}")

if __name__ == "__main__":
    main()