#!/usr/bin/env python3
"""
æ·±å±¤ç›¸è«‡ã‚·ã‚¹ãƒ†ãƒ  - Claude Codeã¨Geminiã®è‡ªå‹•å¯¾è©±
ç´å¾—ã„ãã¾ã§è‡ªå‹•çš„ã«è³ªå•ã‚’é‡ã­ã¦ç²¾åº¦ã‚’å‘ä¸Š
"""

import os
import json
import requests
from datetime import datetime
from typing import Dict, List, Tuple, Optional

class DeepConsultationSystem:
    """æ·±å±¤ç›¸è«‡ã‚·ã‚¹ãƒ†ãƒ  - è‡ªå‹•çš„ã«å¯¾è©±ã‚’é‡ã­ã‚‹"""
    
    def __init__(self):
        self.gemini_api_key = self._load_api_key()
        self.conversation_history = []
        self.max_depth = 5  # æœ€å¤§ç›¸è«‡å›æ•°
        
    def _load_api_key(self) -> str:
        env_file = '/mnt/c/Desktop/Research/.env'
        with open(env_file, 'r') as f:
            for line in f:
                if line.startswith('GEMINI_API_KEY='):
                    return line.split('=', 1)[1].strip().strip('"')
        raise ValueError("GEMINI_API_KEY not found")
    
    def _call_gemini(self, prompt: str) -> str:
        """Gemini APIã‚’å‘¼ã³å‡ºã™"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={self.gemini_api_key}"
        
        headers = {'Content-Type': 'application/json'}
        data = {
            "contents": [{
                "parts": [{"text": prompt}]
            }]
        }
        
        try:
            response = requests.post(url, headers=headers, data=json.dumps(data), timeout=30)
            if response.status_code == 200:
                result = response.json()
                return result['candidates'][0]['content']['parts'][0]['text']
            else:
                return f"ã‚¨ãƒ©ãƒ¼: {response.status_code}"
        except Exception as e:
            return f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def _generate_followup_question(self, response: str, depth: int) -> str:
        """å›ç­”ã«åŸºã¥ã„ã¦è¿½åŠ è³ªå•ã‚’ç”Ÿæˆ"""
        followup_prompt = f"""
ä»¥ä¸‹ã®å›ç­”ã‚’åˆ†æã—ã€ã‚ˆã‚Šæ·±ã„ç†è§£ã®ãŸã‚ã®è¿½åŠ è³ªå•ã‚’1ã¤ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

å›ç­”:
{response}

è¿½åŠ è³ªå•ã®æ¡ä»¶:
1. å…·ä½“çš„ãªæ•°å€¤ã‚„æ ¹æ‹ ã‚’æ±‚ã‚ã‚‹
2. å®Ÿç”¨çš„ãªå¿œç”¨æ–¹æ³•ã‚’æ¢ã‚‹
3. æ½œåœ¨çš„ãªå•é¡Œç‚¹ã‚„é™ç•Œã‚’æ˜ç¢ºã«ã™ã‚‹
4. ä»£æ›¿æ¡ˆã‚„æ¯”è¼ƒå¯¾è±¡ã‚’æ¤œè¨ã™ã‚‹

æ·±ã•ãƒ¬ãƒ™ãƒ«: {depth}/5
ã‚ˆã‚Šæœ¬è³ªçš„ãªè³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""
        return self._call_gemini(followup_prompt)
    
    def _check_convergence(self, responses: List[str]) -> bool:
        """å›ç­”ãŒåæŸã—ãŸã‹ãƒã‚§ãƒƒã‚¯"""
        if len(responses) < 2:
            return False
        
        # æœ€æ–°2ã¤ã®å›ç­”ã‚’æ¯”è¼ƒ
        check_prompt = f"""
ä»¥ä¸‹ã®2ã¤ã®å›ç­”ã‚’æ¯”è¼ƒã—ã€ååˆ†ãªæ·±ã•ã¨å…·ä½“æ€§ã«é”ã—ãŸã‹åˆ¤å®šã—ã¦ãã ã•ã„ï¼š

å›ç­”1:
{responses[-2][:500]}

å›ç­”2:
{responses[-1][:500]}

ä»¥ä¸‹ã®åŸºæº–ã§åˆ¤å®š:
1. å…·ä½“çš„ãªæ•°å€¤ã‚„äº‹ä¾‹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹
2. å®Ÿè·µçš„ãªææ¡ˆãŒã‚ã‚‹ã‹
3. é™ç•Œã‚„æ³¨æ„ç‚¹ãŒæ˜ç¢ºã‹

ååˆ†ãªæ·±ã•ã«é”ã—ãŸå ´åˆã¯ã€ŒåæŸã€ã€ã•ã‚‰ã«æ·±æ˜ã‚ŠãŒå¿…è¦ãªå ´åˆã¯ã€Œç¶™ç¶šã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
        
        result = self._call_gemini(check_prompt)
        return "åæŸ" in result
    
    def deep_consult(self, initial_query: str, context: Dict = None) -> Dict:
        """æ·±å±¤ç›¸è«‡ã‚’å®Ÿè¡Œ"""
        self.conversation_history = []
        responses = []
        questions = [initial_query]
        
        # åˆæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã‚‹
        if context:
            current_query = f"{initial_query}\n\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {json.dumps(context, ensure_ascii=False)}"
        else:
            current_query = initial_query
        
        print(f"ğŸ” æ·±å±¤ç›¸è«‡é–‹å§‹: {initial_query}")
        
        for depth in range(self.max_depth):
            print(f"\nğŸ“Š æ·±åº¦ {depth + 1}/{self.max_depth}")
            
            # Geminiã«è³ªå•
            response = self._call_gemini(current_query)
            responses.append(response)
            
            # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            self.conversation_history.append({
                "depth": depth + 1,
                "question": current_query,
                "response": response,
                "timestamp": datetime.now().isoformat()
            })
            
            print(f"ğŸ’­ è³ªå•: {current_query[:100]}...")
            print(f"ğŸ’¡ å›ç­”: {response[:200]}...")
            
            # åæŸãƒã‚§ãƒƒã‚¯
            if depth >= 1 and self._check_convergence(responses):
                print("âœ… ååˆ†ãªæ·±ã•ã«åˆ°é”ã—ã¾ã—ãŸ")
                break
            
            # æ¬¡ã®è³ªå•ã‚’ç”Ÿæˆ
            if depth < self.max_depth - 1:
                followup = self._generate_followup_question(response, depth + 1)
                questions.append(followup)
                
                # ä¼šè©±ã®æ–‡è„ˆã‚’å«ã‚ãŸè³ªå•
                current_query = f"""
ã“ã‚Œã¾ã§ã®è­°è«–:
è³ªå•: {questions[-2]}
å›ç­”: {response[:300]}...

è¿½åŠ è³ªå•: {followup}
"""
        
        # æœ€çµ‚çš„ãªçµ±åˆåˆ†æ
        final_analysis = self._create_final_analysis(self.conversation_history)
        
        return {
            "initial_query": initial_query,
            "context": context,
            "conversation_depth": len(self.conversation_history),
            "conversation_history": self.conversation_history,
            "final_analysis": final_analysis,
            "timestamp": datetime.now().isoformat()
        }
    
    def _create_final_analysis(self, history: List[Dict]) -> str:
        """å¯¾è©±å±¥æ­´ã‹ã‚‰æœ€çµ‚åˆ†æã‚’ç”Ÿæˆ"""
        summary_prompt = f"""
ä»¥ä¸‹ã®å¯¾è©±å±¥æ­´ã‹ã‚‰ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’çµ±åˆã—ãŸæœ€çµ‚åˆ†æã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

{json.dumps(history, ensure_ascii=False, indent=2)}

ä»¥ä¸‹ã®å½¢å¼ã§ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š
1. ä¸»è¦ãªç™ºè¦‹
2. å®Ÿè·µçš„ãªæ¨å¥¨äº‹é …
3. æ³¨æ„ã™ã¹ãåˆ¶ç´„
4. ä»Šå¾Œã®æ¤œè¨äº‹é …
"""
        
        return self._call_gemini(summary_prompt)
    
    def save_consultation(self, result: Dict, filename: str = None):
        """ç›¸è«‡çµæœã‚’ä¿å­˜"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"deep_consultation_{timestamp}.json"
        
        filepath = os.path.join("/mnt/c/Desktop/Research", filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚‚ç”Ÿæˆ
        report = self._generate_report(result)
        report_path = filepath.replace('.json', '_report.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return filepath, report_path
    
    def _generate_report(self, result: Dict) -> str:
        """ç›¸è«‡çµæœã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""# æ·±å±¤ç›¸è«‡ãƒ¬ãƒãƒ¼ãƒˆ

**åˆæœŸè³ªå•**: {result['initial_query']}  
**ç›¸è«‡æ·±åº¦**: {result['conversation_depth']}å›  
**å®Ÿæ–½æ—¥æ™‚**: {result['timestamp']}

## å¯¾è©±ã®æµã‚Œ

"""
        
        for item in result['conversation_history']:
            report += f"### æ·±åº¦ {item['depth']}\n\n"
            report += f"**è³ªå•**: {item['question']}\n\n"
            report += f"**å›ç­”**: {item['response']}\n\n"
            report += "---\n\n"
        
        report += f"""## æœ€çµ‚çµ±åˆåˆ†æ

{result['final_analysis']}

## ã¾ã¨ã‚

ã“ã®æ·±å±¤ç›¸è«‡ã«ã‚ˆã‚Šã€åˆæœŸã®è³ªå•ã‹ã‚‰{result['conversation_depth']}æ®µéšã®æ·±æ˜ã‚Šã‚’è¡Œã„ã€
ã‚ˆã‚Šå…·ä½“çš„ã§å®Ÿè·µçš„ãªæ´å¯Ÿã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚
"""
        
        return report

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_deep_system = None

def get_deep_system():
    global _deep_system
    if _deep_system is None:
        _deep_system = DeepConsultationSystem()
    return _deep_system

def deep_consult(query: str, context: Dict = None) -> Dict:
    """ç°¡æ˜“ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    system = get_deep_system()
    return system.deep_consult(query, context)

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    result = deep_consult(
        "ç”»åƒåˆ†é¡ã®ç‰¹åŒ–å‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§25.9%ã®ç²¾åº¦å‘ä¸Šã‚’é”æˆã—ã¾ã—ãŸã€‚ã“ã®çµæœã®æ„å‘³ã¯ï¼Ÿ",
        context={"ã‚«ãƒ†ã‚´ãƒªæ•°": 16, "çµ±è¨ˆçš„æœ‰æ„æ€§": "Cohen's d=1.2"}
    )
    
    print("\n" + "="*60)
    print("ğŸ¯ æœ€çµ‚çµ±åˆåˆ†æ:")
    print(result["final_analysis"])